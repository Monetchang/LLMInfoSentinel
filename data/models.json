[
    {
        "title": "deepseek-ai/DeepSeek-V3-0324",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
        "time": "2025-03-25T06:58:36",
        "details": {
            "description": "DeepSeek-V3-0324FeaturesDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.Reasoning CapabilitiesSignificant improvements in benchmark performance:MMLU-Pro: 75.9 → 81.2 (+5.3)GPQA: 59.1 → 68.4 (+9.3)AIME: 39.6 → 59.4 (+19.8)LiveCodeBench: 39.2 → 49.2 (+10.0)Front-End Web DevelopmentImproved the executability of the codeMore aesthetically pleasing web pages and game front-endsChinese Writing ProficiencyEnhanced style and content quality:Aligned with the R1 writing styleBetter quality in medium-to-long-form writingFeature EnhancementsImproved multi-turn interactive rewritingOptimized translation quality and letter writingChinese Search CapabilitiesEnhanced report analysis requests with more detailed outputsFunction Calling ImprovementsIncreased accuracy in Function Calling, fixing issues from previous V3 versionsUsage RecommendationsSystem PromptIn the official DeepSeek web/app, we use the same system prompt with a specific date.该助手为DeepSeek Chat，由深度求索公司创造。\n今天是{current date}。For example,该助手为DeepSeek Chat，由深度求索公司创造。\n今天是3月24日，星期一。TemperatureIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.Tmodel=Tapi×0.3(0≤Tapi≤1)T_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1)Tmodel​=Tapi​×0.3(0≤Tapi​≤1)Tmodel=Tapi−0.7(1<Tapi≤2)T_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2)Tmodel​=Tapi​−0.7(1<Tapi​≤2)Thus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.Prompts for File Uploading and Web SearchFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.file_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"For Web Search, {search_results}, {cur_date}, and {question} are arguments.For Chinese query, we use the prompt:search_answer_zh_template = \\\n'''# 以下内容是基于用户发送的消息的搜索结果:\n{search_results}\n在我给你的搜索结果中，每个结果都是[webpage X begin]...[webpage X end]格式的，X代表每篇文章的数字索引。请在适当的情况下在句子末尾引用上下文。请按照引用编号[citation:X]的格式在答案中对应部分引用上下文。如果一句话源自多个上下文，请列出所有相关的引用编号，例如[citation:3][citation:5]，切记不要将引用集中在最后返回引用编号，而是在答案对应部分列出。\n在回答时，请注意以下几点：\n- 今天是{cur_date}。\n- 并非搜索结果的所有内容都与用户的问题密切相关，你需要结合问题，对搜索结果进行甄别、筛选。\n- 对于列举类的问题（如列举所有航班信息），尽量将答案控制在10个要点以内，并告诉用户可以查看搜索来源、获得完整信息。优先提供信息完整、最相关的列举项；如非必要，不要主动告诉用户搜索结果未提供的内容。\n- 对于创作类的问题（如写论文），请务必在正文的段落中引用对应的参考编号，例如[citation:3][citation:5]，不能只在文章末尾引用。你需要解读并概括用户的题目要求，选择合适的格式，充分利用搜索结果并抽取重要信息，生成符合用户要求、极具思想深度、富有创造力与专业性的答案。你的创作篇幅需要尽可能延长，对于每一个要点的论述要推测用户的意图，给出尽可能多角度的回答要点，且务必信息量大、论述详尽。\n- 如果回答很长，请尽量结构化、分段落总结。如果需要分点作答，尽量控制在5个点以内，并合并相关的内容。\n- 对于客观类的问答，如果问题的答案非常简短，可以适当补充一到两句相关信息，以丰富内容。\n- 你需要根据用户要求和回答内容选择合适、美观的回答格式，确保可读性强。\n- 你的回答应该综合多个相关网页来回答，不能重复引用一个网页。\n- 除非用户要求，否则你回答的语言需要和用户提问的语言保持一致。\n\n# 用户消息为：\n{question}'''For English query, we use the prompt:search_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n\n# The user's message is:\n{question}'''How to Run LocallyThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visitDeepSeek-V3repo for more information about running this model locally.This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer toDeepSeek-V2.5repo.NOTE: Hugging Face's Transformers has not been directly supported yet.LicenseThis repository and the model weights are licensed under theMIT License.Citation@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "time": "2025-02-24T03:32:35",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "time": "2025-02-24T03:32:20",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "time": "2025-02-24T03:32:07",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "time": "2025-02-24T03:31:45",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "time": "2025-02-24T03:31:29",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "time": "2025-02-24T03:31:15",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Zero",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero",
        "time": "2025-02-24T03:30:49",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
        "time": "2025-02-24T03:30:31",
        "details": {
            "description": "DeepSeek-R1Paper Link👁️1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128K🤗 HuggingFaceDeepSeek-R1671B37B128K🤗 HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14B🤗 HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32B🤗 HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instruct🤗 HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-V3",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "time": "2025-02-24T03:29:50",
        "details": {
            "description": "Paper Link👁️1. IntroductionWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.2. Model SummaryArchitecture: Innovative Load Balancing Strategy and Training ObjectiveOn top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n It can also be used for speculative decoding for inference acceleration.Pre-Training: Towards Ultimate Training EfficiencyWe design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.Post-Training: Knowledge Distillation from DeepSeek-R1We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.3. Model DownloadsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-V3-Base671B37B128K🤗 HuggingFaceDeepSeek-V3671B37B128K🤗 HuggingFaceNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:How_to Run_Locally.For developers looking to dive deeper, we recommend exploringREADME_WEIGHTS.mdfor details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.4. Evaluation ResultsBase ModelStandard BenchmarksBenchmark (Metric)# ShotsDeepSeek-V2Qwen2.5 72BLLaMA3.1 405BDeepSeek-V3Architecture-MoEDenseDenseMoE# Activated Params-21B72B405B37B# Total Params-236B72B405B671BEnglishPile-test (BPB)-0.6060.6380.5420.548BBH (EM)3-shot78.879.882.987.5MMLU (Acc.)5-shot78.485.084.487.1MMLU-Redux (Acc.)5-shot75.683.281.386.2MMLU-Pro (Acc.)5-shot51.458.352.864.4DROP (F1)3-shot80.480.686.089.0ARC-Easy (Acc.)25-shot97.698.498.498.9ARC-Challenge (Acc.)25-shot92.294.595.395.3HellaSwag (Acc.)10-shot87.184.889.288.9PIQA (Acc.)0-shot83.982.685.984.7WinoGrande (Acc.)5-shot86.382.385.284.9RACE-Middle (Acc.)5-shot73.168.174.267.1RACE-High (Acc.)5-shot52.650.356.851.3TriviaQA (EM)5-shot80.071.982.782.9NaturalQuestions (EM)5-shot38.633.241.540.0AGIEval (Acc.)0-shot57.575.860.679.6CodeHumanEval (Pass@1)0-shot43.353.054.965.2MBPP (Pass@1)3-shot65.072.668.475.4LiveCodeBench-Base (Pass@1)3-shot11.612.915.519.4CRUXEval-I (Acc.)2-shot52.559.158.567.3CRUXEval-O (Acc.)2-shot49.859.959.969.8MathGSM8K (EM)8-shot81.688.383.589.3MATH (EM)4-shot43.454.449.061.6MGSM (EM)8-shot63.676.269.979.8CMath (EM)3-shot78.784.577.390.7ChineseCLUEWSC (EM)5-shot82.082.583.082.7C-Eval (Acc.)5-shot81.489.272.590.1CMMLU (Acc.)5-shot84.089.573.788.8CMRC (EM)1-shot77.475.876.076.3C3 (Acc.)0-shot77.476.779.778.6CCPM (Acc.)0-shot93.088.578.692.0MultilingualMMMLU-non-English (Acc.)5-shot64.074.873.879.4Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper.Context WindowEvaluation results on theNeedle In A Haystack(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to128K.Chat ModelStandard Benchmarks (Models larger than 67B)Benchmark (Metric)DeepSeek V2-0506DeepSeek V2.5-0905Qwen2.5 72B-Inst.Llama3.1 405B-Inst.Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3ArchitectureMoEMoEDenseDense--MoE# Activated Params21B21B72B405B--37B# Total Params236B236B72B405B--671BEnglishMMLU (EM)78.280.685.388.688.387.288.5MMLU-Redux (EM)77.980.385.686.288.988.089.1MMLU-Pro (EM)58.566.271.673.378.072.675.9DROP (3-shot F1)83.087.876.788.788.383.791.6IF-Eval (Prompt Strict)57.780.684.186.086.584.386.1GPQA-Diamond (Pass@1)35.341.349.051.165.049.959.1SimpleQA (Correct)9.010.29.117.128.438.224.9FRAMES (Acc.)66.965.469.870.072.580.573.3LongBench v2 (Acc.)31.635.439.436.141.048.148.7CodeHumanEval-Mul (Pass@1)69.377.477.377.281.780.582.6LiveCodeBench (Pass@1-COT)18.829.231.128.436.333.440.5LiveCodeBench (Pass@1)20.328.428.730.132.834.237.6Codeforces (Percentile)17.535.624.825.320.323.651.6SWE Verified (Resolved)-22.623.824.550.838.842.0Aider-Edit (Acc.)60.371.665.463.984.272.979.7Aider-Polyglot (Acc.)-18.27.65.845.316.049.6MathAIME 2024 (Pass@1)4.616.723.323.316.09.339.2MATH-500 (EM)56.374.780.073.878.374.690.2CNMO 2024 (Pass@1)2.810.815.96.813.110.843.2ChineseCLUEWSC (EM)89.990.491.484.785.487.990.9C-Eval (EM)78.679.586.161.576.776.086.5C-SimpleQA (Correct)48.554.148.450.451.359.364.8Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.Open Ended Generation EvaluationModelArena-HardAlpacaEval 2.0DeepSeek-V2.5-090576.250.5Qwen2.5-72B-Instruct81.249.1LLaMA-3.1 405B69.340.5GPT-4o-051380.451.1Claude-Sonnet-3.5-102285.252.0DeepSeek-V385.570.0Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.5. Chat Website & API PlatformYou can chat with DeepSeek-V3 on DeepSeek's official website:chat.deepseek.comWe also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:DeepSeek-Infer Demo: We provide a simple and lightweight demo for FP8 and BF16 inference.SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.LMDeploy: Enables efficient FP8 and BF16 inference for local and cloud deployment.TensorRT-LLM: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.vLLM: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.Huawei Ascend NPU: Supports running DeepSeek-V3 on Huawei Ascend devices.Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.Here is an example of converting FP8 weights to BF16:cd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weightsNOTE: Huggingface's Transformers has not been directly supported yet.6.1 Inference with DeepSeek-Infer Demo (example only)Model Weights & Demo Code PreparationFirst, clone our DeepSeek-V3 GitHub repository:git clone https://github.com/deepseek-ai/DeepSeek-V3.gitNavigate to theinferencefolder and install dependencies listed inrequirements.txt.cd DeepSeek-V3/inference\npip install -r requirements.txtDownload the model weights from HuggingFace, and put them into/path/to/DeepSeek-V3folder.Model Weights ConversionConvert HuggingFace model weights to a specific format:python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16RunThen you can chat with DeepSeek-V3:torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200Or batch inference on a given file:torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE6.2 Inference with SGLang (recommended)SGLangcurrently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.Notably,SGLang v0.4.1fully supports running DeepSeek-V3 on bothNVIDIA and AMD GPUs, making it a highly versatile and robust solution.Here are the launch instructions from the SGLang team:https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v36.3 Inference with LMDeploy (recommended)LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here:https://github.com/InternLM/lmdeploy/issues/29606.4 Inference with TRT-LLM (recommended)TensorRT-LLMnow supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly:https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.6.5 Inference with vLLM (recommended)vLLMv0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offerspipeline parallelismallowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to thevLLM instructions. Please feel free to followthe enhancement planas well.6.6 Recommended Inference Functionality with AMD GPUsIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to theSGLang instructions.6.7 Recommended Inference Functionality with Huawei Ascend NPUsTheMindIEframework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow theinstructions here.7. LicenseThis code repository is licensed underthe MIT License. The use of DeepSeek-V3 Base/Chat models is subject tothe Model License. DeepSeek-V3 series (including Base and Chat) supports commercial use.8. Citation@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    }
]