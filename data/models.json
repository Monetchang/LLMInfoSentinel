[
    {
        "title": "deepseek-ai/DeepSeek-V3-0324",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
        "time": "2025-03-25T06:58:36",
        "details": {
            "description": "DeepSeek-V3-0324FeaturesDeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects.Reasoning CapabilitiesSignificant improvements in benchmark performance:MMLU-Pro: 75.9 ‚Üí 81.2 (+5.3)GPQA: 59.1 ‚Üí 68.4 (+9.3)AIME: 39.6 ‚Üí 59.4 (+19.8)LiveCodeBench: 39.2 ‚Üí 49.2 (+10.0)Front-End Web DevelopmentImproved the executability of the codeMore aesthetically pleasing web pages and game front-endsChinese Writing ProficiencyEnhanced style and content quality:Aligned with the R1 writing styleBetter quality in medium-to-long-form writingFeature EnhancementsImproved multi-turn interactive rewritingOptimized translation quality and letter writingChinese Search CapabilitiesEnhanced report analysis requests with more detailed outputsFunction Calling ImprovementsIncreased accuracy in Function Calling, fixing issues from previous V3 versionsUsage RecommendationsSystem PromptIn the official DeepSeek web/app, we use the same system prompt with a specific date.ËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ{current date}„ÄÇFor example,ËØ•Âä©Êâã‰∏∫DeepSeek ChatÔºåÁî±Ê∑±Â∫¶Ê±ÇÁ¥¢ÂÖ¨Âè∏ÂàõÈÄ†„ÄÇ\n‰ªäÂ§©ÊòØ3Êúà24Êó•ÔºåÊòüÊúü‰∏Ä„ÄÇTemperatureIn our web and application environments, the temperature parameter $T_{model}$ is set to 0.3. Because many users use the default temperature 1.0 in API call, we have implemented an API temperature $T_{api}$ mapping mechanism that adjusts the input API temperature value of 1.0 to the most suitable model temperature setting of 0.3.Tmodel=Tapi√ó0.3(0‚â§Tapi‚â§1)T_{model} = T_{api} \\times 0.3 \\quad (0 \\leq T_{api} \\leq 1)Tmodel‚Äã=Tapi‚Äã√ó0.3(0‚â§Tapi‚Äã‚â§1)Tmodel=Tapi‚àí0.7(1<Tapi‚â§2)T_{model} = T_{api} - 0.7 \\quad (1 < T_{api} \\leq 2)Tmodel‚Äã=Tapi‚Äã‚àí0.7(1<Tapi‚Äã‚â§2)Thus, if you call V3 via API, temperature 1.0 equals to the model temperature 0.3.Prompts for File Uploading and Web SearchFor file uploading, please follow the template to create prompts, where {file_name}, {file_content} and {question} are arguments.file_template = \\\n\"\"\"[file name]: {file_name}\n[file content begin]\n{file_content}\n[file content end]\n{question}\"\"\"For Web Search, {search_results}, {cur_date}, and {question} are arguments.For Chinese query, we use the prompt:search_answer_zh_template = \\\n'''# ‰ª•‰∏ãÂÜÖÂÆπÊòØÂü∫‰∫éÁî®Êà∑ÂèëÈÄÅÁöÑÊ∂àÊÅØÁöÑÊêúÁ¥¢ÁªìÊûú:\n{search_results}\nÂú®ÊàëÁªô‰Ω†ÁöÑÊêúÁ¥¢ÁªìÊûú‰∏≠ÔºåÊØè‰∏™ÁªìÊûúÈÉΩÊòØ[webpage X begin]...[webpage X end]Ê†ºÂºèÁöÑÔºåX‰ª£Ë°®ÊØèÁØáÊñáÁ´†ÁöÑÊï∞Â≠óÁ¥¢Âºï„ÄÇËØ∑Âú®ÈÄÇÂΩìÁöÑÊÉÖÂÜµ‰∏ãÂú®Âè•Â≠êÊú´Â∞æÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇËØ∑ÊåâÁÖßÂºïÁî®ÁºñÂè∑[citation:X]ÁöÑÊ†ºÂºèÂú®Á≠îÊ°à‰∏≠ÂØπÂ∫îÈÉ®ÂàÜÂºïÁî®‰∏ä‰∏ãÊñá„ÄÇÂ¶ÇÊûú‰∏ÄÂè•ËØùÊ∫êËá™Â§ö‰∏™‰∏ä‰∏ãÊñáÔºåËØ∑ÂàóÂá∫ÊâÄÊúâÁõ∏ÂÖ≥ÁöÑÂºïÁî®ÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]ÔºåÂàáËÆ∞‰∏çË¶ÅÂ∞ÜÂºïÁî®ÈõÜ‰∏≠Âú®ÊúÄÂêéËøîÂõûÂºïÁî®ÁºñÂè∑ÔºåËÄåÊòØÂú®Á≠îÊ°àÂØπÂ∫îÈÉ®ÂàÜÂàóÂá∫„ÄÇ\nÂú®ÂõûÁ≠îÊó∂ÔºåËØ∑Ê≥®ÊÑè‰ª•‰∏ãÂá†ÁÇπÔºö\n- ‰ªäÂ§©ÊòØ{cur_date}„ÄÇ\n- Âπ∂ÈùûÊêúÁ¥¢ÁªìÊûúÁöÑÊâÄÊúâÂÜÖÂÆπÈÉΩ‰∏éÁî®Êà∑ÁöÑÈóÆÈ¢òÂØÜÂàáÁõ∏ÂÖ≥Ôºå‰Ω†ÈúÄË¶ÅÁªìÂêàÈóÆÈ¢òÔºåÂØπÊêúÁ¥¢ÁªìÊûúËøõË°åÁîÑÂà´„ÄÅÁ≠õÈÄâ„ÄÇ\n- ÂØπ‰∫éÂàó‰∏æÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂàó‰∏æÊâÄÊúâËà™Áè≠‰ø°ÊÅØÔºâÔºåÂ∞ΩÈáèÂ∞ÜÁ≠îÊ°àÊéßÂà∂Âú®10‰∏™Ë¶ÅÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂëäËØâÁî®Êà∑ÂèØ‰ª•Êü•ÁúãÊêúÁ¥¢Êù•Ê∫ê„ÄÅËé∑ÂæóÂÆåÊï¥‰ø°ÊÅØ„ÄÇ‰ºòÂÖàÊèê‰æõ‰ø°ÊÅØÂÆåÊï¥„ÄÅÊúÄÁõ∏ÂÖ≥ÁöÑÂàó‰∏æÈ°πÔºõÂ¶ÇÈùûÂøÖË¶ÅÔºå‰∏çË¶Å‰∏ªÂä®ÂëäËØâÁî®Êà∑ÊêúÁ¥¢ÁªìÊûúÊú™Êèê‰æõÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂàõ‰ΩúÁ±ªÁöÑÈóÆÈ¢òÔºàÂ¶ÇÂÜôËÆ∫ÊñáÔºâÔºåËØ∑Âä°ÂøÖÂú®Ê≠£ÊñáÁöÑÊÆµËêΩ‰∏≠ÂºïÁî®ÂØπÂ∫îÁöÑÂèÇËÄÉÁºñÂè∑Ôºå‰æãÂ¶Ç[citation:3][citation:5]Ôºå‰∏çËÉΩÂè™Âú®ÊñáÁ´†Êú´Â∞æÂºïÁî®„ÄÇ‰Ω†ÈúÄË¶ÅËß£ËØªÂπ∂Ê¶ÇÊã¨Áî®Êà∑ÁöÑÈ¢òÁõÆË¶ÅÊ±ÇÔºåÈÄâÊã©ÂêàÈÄÇÁöÑÊ†ºÂºèÔºåÂÖÖÂàÜÂà©Áî®ÊêúÁ¥¢ÁªìÊûúÂπ∂ÊäΩÂèñÈáçË¶Å‰ø°ÊÅØÔºåÁîüÊàêÁ¨¶ÂêàÁî®Êà∑Ë¶ÅÊ±Ç„ÄÅÊûÅÂÖ∑ÊÄùÊÉ≥Ê∑±Â∫¶„ÄÅÂØåÊúâÂàõÈÄ†Âäõ‰∏é‰∏ì‰∏öÊÄßÁöÑÁ≠îÊ°à„ÄÇ‰Ω†ÁöÑÂàõ‰ΩúÁØáÂπÖÈúÄË¶ÅÂ∞ΩÂèØËÉΩÂª∂ÈïøÔºåÂØπ‰∫éÊØè‰∏Ä‰∏™Ë¶ÅÁÇπÁöÑËÆ∫Ëø∞Ë¶ÅÊé®ÊµãÁî®Êà∑ÁöÑÊÑèÂõæÔºåÁªôÂá∫Â∞ΩÂèØËÉΩÂ§öËßíÂ∫¶ÁöÑÂõûÁ≠îË¶ÅÁÇπÔºå‰∏îÂä°ÂøÖ‰ø°ÊÅØÈáèÂ§ß„ÄÅËÆ∫Ëø∞ËØ¶Â∞Ω„ÄÇ\n- Â¶ÇÊûúÂõûÁ≠îÂæàÈïøÔºåËØ∑Â∞ΩÈáèÁªìÊûÑÂåñ„ÄÅÂàÜÊÆµËêΩÊÄªÁªì„ÄÇÂ¶ÇÊûúÈúÄË¶ÅÂàÜÁÇπ‰ΩúÁ≠îÔºåÂ∞ΩÈáèÊéßÂà∂Âú®5‰∏™ÁÇπ‰ª•ÂÜÖÔºåÂπ∂ÂêàÂπ∂Áõ∏ÂÖ≥ÁöÑÂÜÖÂÆπ„ÄÇ\n- ÂØπ‰∫éÂÆ¢ËßÇÁ±ªÁöÑÈóÆÁ≠îÔºåÂ¶ÇÊûúÈóÆÈ¢òÁöÑÁ≠îÊ°àÈùûÂ∏∏ÁÆÄÁü≠ÔºåÂèØ‰ª•ÈÄÇÂΩìË°•ÂÖÖ‰∏ÄÂà∞‰∏§Âè•Áõ∏ÂÖ≥‰ø°ÊÅØÔºå‰ª•‰∏∞ÂØåÂÜÖÂÆπ„ÄÇ\n- ‰Ω†ÈúÄË¶ÅÊ†πÊçÆÁî®Êà∑Ë¶ÅÊ±ÇÂíåÂõûÁ≠îÂÜÖÂÆπÈÄâÊã©ÂêàÈÄÇ„ÄÅÁæéËßÇÁöÑÂõûÁ≠îÊ†ºÂºèÔºåÁ°Æ‰øùÂèØËØªÊÄßÂº∫„ÄÇ\n- ‰Ω†ÁöÑÂõûÁ≠îÂ∫îËØ•ÁªºÂêàÂ§ö‰∏™Áõ∏ÂÖ≥ÁΩëÈ°µÊù•ÂõûÁ≠îÔºå‰∏çËÉΩÈáçÂ§çÂºïÁî®‰∏Ä‰∏™ÁΩëÈ°µ„ÄÇ\n- Èô§ÈùûÁî®Êà∑Ë¶ÅÊ±ÇÔºåÂê¶Âàô‰Ω†ÂõûÁ≠îÁöÑËØ≠Ë®ÄÈúÄË¶ÅÂíåÁî®Êà∑ÊèêÈóÆÁöÑËØ≠Ë®Ä‰øùÊåÅ‰∏ÄËá¥„ÄÇ\n\n# Áî®Êà∑Ê∂àÊÅØ‰∏∫Ôºö\n{question}'''For English query, we use the prompt:search_answer_en_template = \\\n'''# The following contents are the search results related to the user's message:\n{search_results}\nIn the search results I provide to you, each result is formatted as [webpage X begin]...[webpage X end], where X represents the numerical index of each article. Please cite the context at the end of the relevant sentence when appropriate. Use the citation format [citation:X] in the corresponding part of your answer. If a sentence is derived from multiple contexts, list all relevant citation numbers, such as [citation:3][citation:5]. Be sure not to cluster all citations at the end; instead, include them in the corresponding parts of the answer.\nWhen responding, please keep the following points in mind:\n- Today is {cur_date}.\n- Not all content in the search results is closely related to the user's question. You need to evaluate and filter the search results based on the question.\n- For listing-type questions (e.g., listing all flight information), try to limit the answer to 10 key points and inform the user that they can refer to the search sources for complete information. Prioritize providing the most complete and relevant items in the list. Avoid mentioning content not provided in the search results unless necessary.\n- For creative tasks (e.g., writing an essay), ensure that references are cited within the body of the text, such as [citation:3][citation:5], rather than only at the end of the text. You need to interpret and summarize the user's requirements, choose an appropriate format, fully utilize the search results, extract key information, and generate an answer that is insightful, creative, and professional. Extend the length of your response as much as possible, addressing each point in detail and from multiple perspectives, ensuring the content is rich and thorough.\n- If the response is lengthy, structure it well and summarize it in paragraphs. If a point-by-point format is needed, try to limit it to 5 points and merge related content.\n- For objective Q&A, if the answer is very brief, you may add one or two related sentences to enrich the content.\n- Choose an appropriate and visually appealing format for your response based on the user's requirements and the content of the answer, ensuring strong readability.\n- Your answer should synthesize information from multiple relevant webpages and avoid repeatedly citing the same webpage.\n- Unless the user requests otherwise, your response should be in the same language as the user's question.\n\n# The user's message is:\n{question}'''How to Run LocallyThe model structure of DeepSeek-V3-0324 is exactly the same as DeepSeek-V3. Please visitDeepSeek-V3repo for more information about running this model locally.This model supports features such as function calling, JSON output, and FIM completion. For instructions on how to construct prompts to use these features, please refer toDeepSeek-V2.5repo.NOTE: Hugging Face's Transformers has not been directly supported yet.LicenseThis repository and the model weights are licensed under theMIT License.Citation@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
        "time": "2025-02-24T03:32:35",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
        "time": "2025-02-24T03:32:20",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B",
        "time": "2025-02-24T03:32:07",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "time": "2025-02-24T03:31:45",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B",
        "time": "2025-02-24T03:31:29",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
        "time": "2025-02-24T03:31:15",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1-Zero",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Zero",
        "time": "2025-02-24T03:30:49",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-R1",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
        "time": "2025-02-24T03:30:31",
        "details": {
            "description": "DeepSeek-R1Paper LinküëÅÔ∏è1. IntroductionWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. \nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrated remarkable performance on reasoning.\nWith RL, DeepSeek-R1-Zero naturally emerged with numerous powerful and interesting reasoning behaviors.\nHowever, DeepSeek-R1-Zero encounters challenges such as endless repetition, poor readability, and language mixing. To address these issues and further enhance reasoning performance,\nwe introduce DeepSeek-R1, which incorporates cold-start data before RL.\nDeepSeek-R1 achieves performance comparable to OpenAI-o1 across math, code, and reasoning tasks. \nTo support the research community, we have open-sourced DeepSeek-R1-Zero, DeepSeek-R1, and six dense models distilled from DeepSeek-R1 based on Llama and Qwen. DeepSeek-R1-Distill-Qwen-32B outperforms OpenAI-o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.NOTE: Before running DeepSeek-R1 series models locally, we kindly recommend reviewing theUsage Recommendationsection.2. Model SummaryPost-Training: Large-Scale Reinforcement Learning on the Base ModelWe directly apply reinforcement learning (RL) to the base model without relying on supervised fine-tuning (SFT) as a preliminary step. This approach allows the model to explore chain-of-thought (CoT) for solving complex problems, resulting in the development of DeepSeek-R1-Zero. DeepSeek-R1-Zero demonstrates capabilities such as self-verification, reflection, and generating long CoTs, marking a significant milestone for the research community. Notably, it is the first open research to validate that reasoning capabilities of LLMs can be incentivized purely through RL, without the need for SFT. This breakthrough paves the way for future advancements in this area.We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stages aimed at discovering improved reasoning patterns and aligning with human preferences, as well as two SFT stages that serve as the seed for the model's reasoning and non-reasoning capabilities.\nWe believe the pipeline will benefit the industry by creating better models.Distillation: Smaller Models Can Be Powerful TooWe demonstrate that the reasoning patterns of larger models can be distilled into smaller models, resulting in better performance compared to the reasoning patterns discovered through RL on small models. The open source DeepSeek-R1, as well as its API, will benefit the research community to distill better smaller models in the future.Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models that are widely used in the research community. The evaluation results demonstrate that the distilled smaller dense models perform exceptionally well on benchmarks. We open-source distilled 1.5B, 7B, 8B, 14B, 32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.3. Model DownloadsDeepSeek-R1 ModelsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-R1-Zero671B37B128Kü§ó HuggingFaceDeepSeek-R1671B37B128Kü§ó HuggingFaceDeepSeek-R1-Zero & DeepSeek-R1 are trained based on DeepSeek-V3-Base. \nFor more details regarding the model architecture, please refer toDeepSeek-V3repository.DeepSeek-R1-Distill ModelsModelBase ModelDownloadDeepSeek-R1-Distill-Qwen-1.5BQwen2.5-Math-1.5Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-7BQwen2.5-Math-7Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-8BLlama-3.1-8Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-14BQwen2.5-14Bü§ó HuggingFaceDeepSeek-R1-Distill-Qwen-32BQwen2.5-32Bü§ó HuggingFaceDeepSeek-R1-Distill-Llama-70BLlama-3.3-70B-Instructü§ó HuggingFaceDeepSeek-R1-Distill models are fine-tuned based on open-source models, using samples generated by DeepSeek-R1.\nWe slightly change their configs and tokenizers. Please use our setting to run these models.4. Evaluation ResultsDeepSeek-R1-EvaluationFor all our models, the maximum generation length is set to 32,768 tokens. For benchmarks requiring sampling, we use a temperature of $0.6$, a top-p value of $0.95$, and generate 64 responses per query to estimate pass@1.CategoryBenchmark (Metric)Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3OpenAI o1-miniOpenAI o1-1217DeepSeek R1Architecture--MoE--MoE# Activated Params--37B--37B# Total Params--671B--671BEnglishMMLU (Pass@1)88.387.288.585.291.890.8MMLU-Redux (EM)88.988.089.186.7-92.9MMLU-Pro (EM)78.072.675.980.3-84.0DROP (3-shot F1)88.383.791.683.990.292.2IF-Eval (Prompt Strict)86.584.386.184.8-83.3GPQA-Diamond (Pass@1)65.049.959.160.075.771.5SimpleQA (Correct)28.438.224.97.047.030.1FRAMES (Acc.)72.580.573.376.9-82.5AlpacaEval2.0 (LC-winrate)52.051.170.057.8-87.6ArenaHard (GPT-4-1106)85.280.485.592.0-92.3CodeLiveCodeBench (Pass@1-COT)33.834.2-53.863.465.9Codeforces (Percentile)20.323.658.793.496.696.3Codeforces (Rating)7177591134182020612029SWE Verified (Resolved)50.838.842.041.648.949.2Aider-Polyglot (Acc.)45.316.049.632.961.753.3MathAIME 2024 (Pass@1)16.09.339.263.679.279.8MATH-500 (Pass@1)78.374.690.290.096.497.3CNMO 2024 (Pass@1)13.110.843.267.6-78.8ChineseCLUEWSC (EM)85.487.990.989.9-92.8C-Eval (EM)76.776.086.568.9-91.8C-SimpleQA (Correct)55.458.768.040.3-63.7Distilled Model EvaluationModelAIME 2024 pass@1AIME 2024 cons@64MATH-500 pass@1GPQA Diamond pass@1LiveCodeBench pass@1CodeForces ratingGPT-4o-05139.313.474.649.932.9759Claude-3.5-Sonnet-102216.026.778.365.038.9717o1-mini63.680.090.060.053.81820QwQ-32B-Preview44.060.090.654.541.91316DeepSeek-R1-Distill-Qwen-1.5B28.952.783.933.816.9954DeepSeek-R1-Distill-Qwen-7B55.583.392.849.137.61189DeepSeek-R1-Distill-Qwen-14B69.780.093.959.153.11481DeepSeek-R1-Distill-Qwen-32B72.683.394.362.157.21691DeepSeek-R1-Distill-Llama-8B50.480.089.149.039.61205DeepSeek-R1-Distill-Llama-70B70.086.794.565.257.516335. Chat Website & API PlatformYou can chat with DeepSeek-R1 on DeepSeek's official website:chat.deepseek.com, and switch on the button \"DeepThink\"We also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-R1 ModelsPlease visitDeepSeek-V3repo for more information about running DeepSeek-R1 locally.NOTE: Hugging Face's Transformers has not been directly supported yet.DeepSeek-R1-Distill ModelsDeepSeek-R1-Distill models can be utilized in the same manner as Qwen or Llama models.For instance, you can easily start a service usingvLLM:vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --tensor-parallel-size 2 --max-model-len 32768 --enforce-eagerYou can also easily start a service usingSGLangpython3 -m sglang.launch_server --model deepseek-ai/DeepSeek-R1-Distill-Qwen-32B --trust-remote-code --tp 2Usage RecommendationsWe recommend adhering to the following configurations when utilizing the DeepSeek-R1 series models, including benchmarking, to achieve the expected performance:Set the temperature within the range of 0.5-0.7 (0.6 is recommended) to prevent endless repetitions or incoherent outputs.Avoid adding a system prompt; all instructions should be contained within the user prompt.For mathematical problems, it is advisable to include a directive in your prompt such as: \"Please reason step by step, and put your final answer within \\boxed{}.\"When evaluating model performance, it is recommended to conduct multiple tests and average the results.Additionally, we have observed that the DeepSeek-R1 series models tend to bypass thinking pattern (i.e., outputting \"<think>\\n\\n</think>\") when responding to certain queries, which can adversely affect the model's performance.To ensure that the model engages in thorough reasoning, we recommend enforcing the model to initiate its response with \"<think>\\n\" at the beginning of every output.7. LicenseThis code repository and the model weights are licensed under theMIT License.\nDeepSeek-R1 series support commercial use, allow for any modifications and derivative works, including, but not limited to, distillation for training other LLMs. Please note that:DeepSeek-R1-Distill-Qwen-1.5B, DeepSeek-R1-Distill-Qwen-7B, DeepSeek-R1-Distill-Qwen-14B and DeepSeek-R1-Distill-Qwen-32B are derived fromQwen-2.5 series, which are originally licensed underApache 2.0 License, and now finetuned with 800k samples curated with DeepSeek-R1.DeepSeek-R1-Distill-Llama-8B is derived from Llama3.1-8B-Base and is originally licensed underllama3.1 license.DeepSeek-R1-Distill-Llama-70B is derived from Llama3.3-70B-Instruct and is originally licensed underllama3.3 license.8. Citation@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,\n      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, \n      author={DeepSeek-AI},\n      year={2025},\n      eprint={2501.12948},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2501.12948}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    },
    {
        "title": "deepseek-ai/DeepSeek-V3",
        "link": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "time": "2025-02-24T03:29:50",
        "details": {
            "description": "Paper LinküëÅÔ∏è1. IntroductionWe present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. \nTo achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. \nFurthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. \nWe pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. \nComprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models.\nDespite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training.\nIn addition, its training process is remarkably stable. \nThroughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks.2. Model SummaryArchitecture: Innovative Load Balancing Strategy and Training ObjectiveOn top of the efficient architecture of DeepSeek-V2, we pioneer an auxiliary-loss-free strategy for load balancing, which minimizes the performance degradation that arises from encouraging load balancing.We investigate a Multi-Token Prediction (MTP) objective and prove it beneficial to model performance. \n It can also be used for speculative decoding for inference acceleration.Pre-Training: Towards Ultimate Training EfficiencyWe design an FP8 mixed precision training framework and, for the first time, validate the feasibility and effectiveness of FP8 training on an extremely large-scale model.Through co-design of algorithms, frameworks, and hardware, we overcome the communication bottleneck in cross-node MoE training, nearly achieving full computation-communication overlap.This significantly enhances our training efficiency and reduces the training costs, enabling us to further scale up the model size without additional overhead.At an economical cost of only 2.664M H800 GPU hours, we complete the pre-training of DeepSeek-V3 on 14.8T tokens, producing the currently strongest open-source base model. The subsequent training stages after pre-training require only 0.1M GPU hours.Post-Training: Knowledge Distillation from DeepSeek-R1We introduce an innovative methodology to distill reasoning capabilities from the long-Chain-of-Thought (CoT) model, specifically from one of the DeepSeek R1 series models, into standard LLMs, particularly DeepSeek-V3. Our pipeline elegantly incorporates the verification and reflection patterns of R1 into DeepSeek-V3 and notably improves its reasoning performance. Meanwhile, we also maintain a control over the output style and length of DeepSeek-V3.3. Model DownloadsModel#Total Params#Activated ParamsContext LengthDownloadDeepSeek-V3-Base671B37B128Kü§ó HuggingFaceDeepSeek-V3671B37B128Kü§ó HuggingFaceNOTE: The total size of DeepSeek-V3 models on HuggingFace is 685B, which includes 671B of the Main Model weights and 14B of the Multi-Token Prediction (MTP) Module weights.To ensure optimal performance and flexibility, we have partnered with open-source communities and hardware vendors to provide multiple ways to run the model locally. For step-by-step guidance, check out Section 6:How_to Run_Locally.For developers looking to dive deeper, we recommend exploringREADME_WEIGHTS.mdfor details on the Main Model weights and the Multi-Token Prediction (MTP) Modules. Please note that MTP support is currently under active development within the community, and we welcome your contributions and feedback.4. Evaluation ResultsBase ModelStandard BenchmarksBenchmark (Metric)# ShotsDeepSeek-V2Qwen2.5 72BLLaMA3.1 405BDeepSeek-V3Architecture-MoEDenseDenseMoE# Activated Params-21B72B405B37B# Total Params-236B72B405B671BEnglishPile-test (BPB)-0.6060.6380.5420.548BBH (EM)3-shot78.879.882.987.5MMLU (Acc.)5-shot78.485.084.487.1MMLU-Redux (Acc.)5-shot75.683.281.386.2MMLU-Pro (Acc.)5-shot51.458.352.864.4DROP (F1)3-shot80.480.686.089.0ARC-Easy (Acc.)25-shot97.698.498.498.9ARC-Challenge (Acc.)25-shot92.294.595.395.3HellaSwag (Acc.)10-shot87.184.889.288.9PIQA (Acc.)0-shot83.982.685.984.7WinoGrande (Acc.)5-shot86.382.385.284.9RACE-Middle (Acc.)5-shot73.168.174.267.1RACE-High (Acc.)5-shot52.650.356.851.3TriviaQA (EM)5-shot80.071.982.782.9NaturalQuestions (EM)5-shot38.633.241.540.0AGIEval (Acc.)0-shot57.575.860.679.6CodeHumanEval (Pass@1)0-shot43.353.054.965.2MBPP (Pass@1)3-shot65.072.668.475.4LiveCodeBench-Base (Pass@1)3-shot11.612.915.519.4CRUXEval-I (Acc.)2-shot52.559.158.567.3CRUXEval-O (Acc.)2-shot49.859.959.969.8MathGSM8K (EM)8-shot81.688.383.589.3MATH (EM)4-shot43.454.449.061.6MGSM (EM)8-shot63.676.269.979.8CMath (EM)3-shot78.784.577.390.7ChineseCLUEWSC (EM)5-shot82.082.583.082.7C-Eval (Acc.)5-shot81.489.272.590.1CMMLU (Acc.)5-shot84.089.573.788.8CMRC (EM)1-shot77.475.876.076.3C3 (Acc.)0-shot77.476.779.778.6CCPM (Acc.)0-shot93.088.578.692.0MultilingualMMMLU-non-English (Acc.)5-shot64.074.873.879.4Note: Best results are shown in bold. Scores with a gap not exceeding 0.3 are considered to be at the same level. DeepSeek-V3 achieves the best performance on most benchmarks, especially on math and code tasks.\nFor more evaluation details, please check our paper.Context WindowEvaluation results on theNeedle In A Haystack(NIAH) tests.  DeepSeek-V3 performs well across all context window lengths up to128K.Chat ModelStandard Benchmarks (Models larger than 67B)Benchmark (Metric)DeepSeek V2-0506DeepSeek V2.5-0905Qwen2.5 72B-Inst.Llama3.1 405B-Inst.Claude-3.5-Sonnet-1022GPT-4o 0513DeepSeek V3ArchitectureMoEMoEDenseDense--MoE# Activated Params21B21B72B405B--37B# Total Params236B236B72B405B--671BEnglishMMLU (EM)78.280.685.388.688.387.288.5MMLU-Redux (EM)77.980.385.686.288.988.089.1MMLU-Pro (EM)58.566.271.673.378.072.675.9DROP (3-shot F1)83.087.876.788.788.383.791.6IF-Eval (Prompt Strict)57.780.684.186.086.584.386.1GPQA-Diamond (Pass@1)35.341.349.051.165.049.959.1SimpleQA (Correct)9.010.29.117.128.438.224.9FRAMES (Acc.)66.965.469.870.072.580.573.3LongBench v2 (Acc.)31.635.439.436.141.048.148.7CodeHumanEval-Mul (Pass@1)69.377.477.377.281.780.582.6LiveCodeBench (Pass@1-COT)18.829.231.128.436.333.440.5LiveCodeBench (Pass@1)20.328.428.730.132.834.237.6Codeforces (Percentile)17.535.624.825.320.323.651.6SWE Verified (Resolved)-22.623.824.550.838.842.0Aider-Edit (Acc.)60.371.665.463.984.272.979.7Aider-Polyglot (Acc.)-18.27.65.845.316.049.6MathAIME 2024 (Pass@1)4.616.723.323.316.09.339.2MATH-500 (EM)56.374.780.073.878.374.690.2CNMO 2024 (Pass@1)2.810.815.96.813.110.843.2ChineseCLUEWSC (EM)89.990.491.484.785.487.990.9C-Eval (EM)78.679.586.161.576.776.086.5C-SimpleQA (Correct)48.554.148.450.451.359.364.8Note: All models are evaluated in a configuration that limits the output length to 8K. Benchmarks containing fewer than 1000 samples are tested multiple times using varying temperature settings to derive robust final results. DeepSeek-V3 stands as the best-performing open-source model, and also exhibits competitive performance against frontier closed-source models.Open Ended Generation EvaluationModelArena-HardAlpacaEval 2.0DeepSeek-V2.5-090576.250.5Qwen2.5-72B-Instruct81.249.1LLaMA-3.1 405B69.340.5GPT-4o-051380.451.1Claude-Sonnet-3.5-102285.252.0DeepSeek-V385.570.0Note: English open-ended conversation evaluations. For AlpacaEval 2.0, we use the length-controlled win rate as the metric.5. Chat Website & API PlatformYou can chat with DeepSeek-V3 on DeepSeek's official website:chat.deepseek.comWe also provide OpenAI-Compatible API at DeepSeek Platform:platform.deepseek.com6. How to Run LocallyDeepSeek-V3 can be deployed locally using the following hardware and open-source community software:DeepSeek-Infer Demo: We provide a simple and lightweight demo for FP8 and BF16 inference.SGLang: Fully support the DeepSeek-V3 model in both BF16 and FP8 inference modes.LMDeploy: Enables efficient FP8 and BF16 inference for local and cloud deployment.TensorRT-LLM: Currently supports BF16 inference and INT4/8 quantization, with FP8 support coming soon.vLLM: Support DeekSeek-V3 model with FP8 and BF16 modes for tensor parallelism and pipeline parallelism.AMD GPU: Enables running the DeepSeek-V3 model on AMD GPUs via SGLang in both BF16 and FP8 modes.Huawei Ascend NPU: Supports running DeepSeek-V3 on Huawei Ascend devices.Since FP8 training is natively adopted in our framework, we only provide FP8 weights. If you require BF16 weights for experimentation, you can use the provided conversion script to perform the transformation.Here is an example of converting FP8 weights to BF16:cd inference\npython fp8_cast_bf16.py --input-fp8-hf-path /path/to/fp8_weights --output-bf16-hf-path /path/to/bf16_weightsNOTE: Huggingface's Transformers has not been directly supported yet.6.1 Inference with DeepSeek-Infer Demo (example only)Model Weights & Demo Code PreparationFirst, clone our DeepSeek-V3 GitHub repository:git clone https://github.com/deepseek-ai/DeepSeek-V3.gitNavigate to theinferencefolder and install dependencies listed inrequirements.txt.cd DeepSeek-V3/inference\npip install -r requirements.txtDownload the model weights from HuggingFace, and put them into/path/to/DeepSeek-V3folder.Model Weights ConversionConvert HuggingFace model weights to a specific format:python convert.py --hf-ckpt-path /path/to/DeepSeek-V3 --save-path /path/to/DeepSeek-V3-Demo --n-experts 256 --model-parallel 16RunThen you can chat with DeepSeek-V3:torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --interactive --temperature 0.7 --max-new-tokens 200Or batch inference on a given file:torchrun --nnodes 2 --nproc-per-node 8 generate.py --node-rank $RANK --master-addr $ADDR --ckpt-path /path/to/DeepSeek-V3-Demo --config configs/config_671B.json --input-file $FILE6.2 Inference with SGLang (recommended)SGLangcurrently supports MLA optimizations, FP8 (W8A8), FP8 KV Cache, and Torch Compile, delivering state-of-the-art latency and throughput performance among open-source frameworks.Notably,SGLang v0.4.1fully supports running DeepSeek-V3 on bothNVIDIA and AMD GPUs, making it a highly versatile and robust solution.Here are the launch instructions from the SGLang team:https://github.com/sgl-project/sglang/tree/main/benchmark/deepseek_v36.3 Inference with LMDeploy (recommended)LMDeploy, a flexible and high-performance inference and serving framework tailored for large language models, now supports DeepSeek-V3. It offers both offline pipeline processing and online deployment capabilities, seamlessly integrating with PyTorch-based workflows.For comprehensive step-by-step instructions on running DeepSeek-V3 with LMDeploy, please refer to here:https://github.com/InternLM/lmdeploy/issues/29606.4 Inference with TRT-LLM (recommended)TensorRT-LLMnow supports the DeepSeek-V3 model, offering precision options such as BF16 and INT4/INT8 weight-only. Support for FP8 is currently in progress and will be released soon. You can access the custom branch of TRTLLM specifically for DeepSeek-V3 support through the following link to experience the new features directly:https://github.com/NVIDIA/TensorRT-LLM/tree/deepseek/examples/deepseek_v3.6.5 Inference with vLLM (recommended)vLLMv0.6.6 supports DeepSeek-V3 inference for FP8 and BF16 modes on both NVIDIA and AMD GPUs. Aside from standard techniques, vLLM offerspipeline parallelismallowing you to run this model on multiple machines connected by networks. For detailed guidance, please refer to thevLLM instructions. Please feel free to followthe enhancement planas well.6.6 Recommended Inference Functionality with AMD GPUsIn collaboration with the AMD team, we have achieved Day-One support for AMD GPUs using SGLang, with full compatibility for both FP8 and BF16 precision. For detailed guidance, please refer to theSGLang instructions.6.7 Recommended Inference Functionality with Huawei Ascend NPUsTheMindIEframework from the Huawei Ascend community has successfully adapted the BF16 version of DeepSeek-V3. For step-by-step guidance on Ascend NPUs, please follow theinstructions here.7. LicenseThis code repository is licensed underthe MIT License. The use of DeepSeek-V3 Base/Chat models is subject tothe Model License. DeepSeek-V3 series (including Base and Chat) supports commercial use.8. Citation@misc{deepseekai2024deepseekv3technicalreport,\n      title={DeepSeek-V3 Technical Report}, \n      author={DeepSeek-AI},\n      year={2024},\n      eprint={2412.19437},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2412.19437}, \n}9. ContactIf you have any questions, please raise an issue or contact us atservice@deepseek.com."
        }
    }
]